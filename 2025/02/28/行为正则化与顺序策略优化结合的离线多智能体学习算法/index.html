<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>&#39;行为正则化与顺序策略优化结合的离线多智能体学习算法&#39; | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="离线多智能体强化学习（MARL）是一个新兴领域，目标是在从预先收集的数据集中学习最佳的多智能体策略。相比于单智能体情况，多智能体环境涉及到大规模的联合状态——动作空间和多智能体间的耦合行为，这给离线策略优化带来了额外的复杂性。随着人工智能技术的发展，多智能体系统在诸如自动驾驶、智能家居、机器人协作以及智能调度决策等方面展现了巨大的应用潜力。但是离线MARL较单智能体情况下更加复杂，其涉及庞大的联合">
<meta property="og:type" content="article">
<meta property="og:title" content="&#39;行为正则化与顺序策略优化结合的离线多智能体学习算法&#39;">
<meta property="og:url" content="http://example.com/2025/02/28/%E8%A1%8C%E4%B8%BA%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E9%A1%BA%E5%BA%8F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%BB%93%E5%90%88%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="离线多智能体强化学习（MARL）是一个新兴领域，目标是在从预先收集的数据集中学习最佳的多智能体策略。相比于单智能体情况，多智能体环境涉及到大规模的联合状态——动作空间和多智能体间的耦合行为，这给离线策略优化带来了额外的复杂性。随着人工智能技术的发展，多智能体系统在诸如自动驾驶、智能家居、机器人协作以及智能调度决策等方面展现了巨大的应用潜力。但是离线MARL较单智能体情况下更加复杂，其涉及庞大的联合">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/8bae7287de099ebc2a918d8f6986365636496.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/6d2cb2487ad38c1fc6113eff1e93b1ce120808.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/b38b6e530f683abb2fa295ce3c01dece44952.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/2284bfa1796b4795d9018e4bf5f7e9be33448.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/811a42ca80594b7ac18f74d2d0ea0d57351242.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/8cace7a43473e6a86f738daa264ff77e47957.png">
<meta property="og:image" content="https://p0.meituan.net/meituantechblog/1d7c0a491a654b42af0c7ead460237fe52663.png">
<meta property="article:published_time" content="2025-02-28T09:25:04.000Z">
<meta property="article:modified_time" content="2025-02-28T09:25:56.678Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p0.meituan.net/meituantechblog/8bae7287de099ebc2a918d8f6986365636496.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-行为正则化与顺序策略优化结合的离线多智能体学习算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/02/28/%E8%A1%8C%E4%B8%BA%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E9%A1%BA%E5%BA%8F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%BB%93%E5%90%88%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2025-02-28T09:25:04.000Z" itemprop="datePublished">2025-02-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      &#39;行为正则化与顺序策略优化结合的离线多智能体学习算法&#39;
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>离线多智能体强化学习（MARL）是一个新兴领域，目标是在从预先收集的数据集中学习最佳的多智能体策略。相比于单智能体情况，多智能体环境涉及到大规模的联合状态——动作空间和多智能体间的耦合行为，这给离线策略优化带来了额外的复杂性。随着人工智能技术的发展，多智能体系统在诸如自动驾驶、智能家居、机器人协作以及智能调度决策等方面展现了巨大的应用潜力。但是离线MARL较单智能体情况下更加复杂，其涉及庞大的联合状态-动作空间和多智能体间的复杂互动行为，这使得离线策略优化成为一项艰巨的任务。</p>
<p>离线MARL面临的主要挑战包括：一是如何有效应对分布偏移问题，即在策略评估过程中，分布外（OOD）样本可能导致误差积累；二是在多智能体环境下，协调多个智能体的行为显得尤为困难。现有的离线MARL方法尽管取得了一些进展，但仍存在不协调行为和分布外联合动作的问题。为了应对这些挑战，中山大学计算机学院、美团履约平台技术部开展了学术合作项目，联合提出了一种新颖的离线MARL算法——样本内顺序策略优化（In-Sample Sequential Policy Optimization, InSPO），该方法通过顺序更新每个智能体的策略，避免选择OOD联合动作，同时增强了智能体之间的协调性。</p>
<p>2024年12月11 日，中山大学计算机学院的刘宗凯、林谦、余超和伍夏威等学术界的专家和教授，跟美团技术团队联合发表了一篇技术论文《Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization》（<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.07639">论文下载</a>）。研究团队的主要贡献在于提出了InSPO算法，该算法不仅避免了OOD联合动作，还通过探索行为策略中的低概率动作，解决了提前收敛到次优解的问题。理论上InSPO保证了策略的单调改进，并收敛到量化响应均衡（QRE）。实验结果表明，InSPO在多个离线MARL任务中表现出了显著的效果，与当前最先进的方法相比具有明显的优势。</p>
<h2 id="合作型马尔可夫博弈"><a href="#合作型马尔可夫博弈" class="headerlink" title="合作型马尔可夫博弈"></a>合作型马尔可夫博弈</h2><p>在理解离线多智能体强化学习（MARL）中的样本内顺序策略优化之前，首先需要了解合作型马尔可夫博弈的基本概念和框架。</p>
<p><img src="https://p0.meituan.net/meituantechblog/8bae7287de099ebc2a918d8f6986365636496.png" alt="图1：XOR博弈（a）是联合行动的奖励矩阵（b）是数据集的分布"></p>
<p>图1：XOR博弈（a）是联合行动的奖励矩阵（b）是数据集的分布</p>
<h3 id="定义与基本概念"><a href="#定义与基本概念" class="headerlink" title="定义与基本概念"></a>定义与基本概念</h3><p><img src="https://p0.meituan.net/meituantechblog/6d2cb2487ad38c1fc6113eff1e93b1ce120808.png" alt="img"></p>
<h3 id="IGM原则与值分解"><a href="#IGM原则与值分解" class="headerlink" title="IGM原则与值分解"></a>IGM原则与值分解</h3><p>在多智能体系统中，直接计算联合Q函数是一个极其复杂的问题，因为状态-动作空间会随着智能体数量的增加而指数级增长。值分解方法通过将联合Q函数分解为每个智能体的个体Q函数，极大地简化了这个计算过程。具体来说，联合Q函数Q(s,a)被表示为每个智能体Q函数QiQi的组合。这种分解方式依赖于个体-全局-最大化（IGM）原则，即最优联合动作可以通过每个智能体的贪婪动作来识别。然而，这种方法在处理环境中存在多模态奖励图谱时可能会遇到困难，因为IGM假设往往会被破坏。</p>
<p><img src="https://p0.meituan.net/meituantechblog/b38b6e530f683abb2fa295ce3c01dece44952.png" alt="图2：M-NE博弈（a）是联合行动的奖励矩阵（b）是数据集的分布"></p>
<p>图2：M-NE博弈（a）是联合行动的奖励矩阵（b）是数据集的分布</p>
<h3 id="离线MARL中的行为正则化马尔可夫博弈"><a href="#离线MARL中的行为正则化马尔可夫博弈" class="headerlink" title="离线MARL中的行为正则化马尔可夫博弈"></a>离线MARL中的行为正则化马尔可夫博弈</h3><p>为了有效应对离线MARL中的分布偏移问题，行为正则化马尔可夫博弈引入了一个与数据相关的正则化项。这个正则化项通过在奖励函数中加入额外的惩罚，迫使学习到的策略尽量接近行为策略，从而避免选择分布外的动作。在这个框架中，策略的目标是最大化期望折扣回报，同时减去正则化项，以此平衡策略的探索和利用。这样不仅提高了策略的稳定性，还能防止其收敛到局部最优解。</p>
<p>通过引入这些基础概念和原理，合作型马尔可夫博弈为多智能体系统的行为建模和优化提供了一个强大的工具。尤其在离线环境中，结合行为正则化和值分解方法，可以有效解决多智能体间的协调问题，并提高策略的整体表现。</p>
<h2 id="样本内顺序策略优化"><a href="#样本内顺序策略优化" class="headerlink" title="样本内顺序策略优化"></a>样本内顺序策略优化</h2><p>在离线多智能体强化学习（MARL）中，策略的优化往往面临着分布外（OOD）联合动作和局部最优解问题。为了应对这些挑战，研究团队提出了一种创新的方法——样本内顺序策略优化（In-Sample Sequential Policy Optimization, InSPO）。该方法在行为正则化马尔可夫博弈框架下进行，结合了逆KL散度和最大熵正则化，旨在通过顺序更新每个智能体的策略，避免选择OOD联合动作，同时增强智能体之间的协调。</p>
<h3 id="样本内顺序策略优化的数学推导"><a href="#样本内顺序策略优化的数学推导" class="headerlink" title="样本内顺序策略优化的数学推导"></a>样本内顺序策略优化的数学推导</h3><p>InSPO方法的核心在于通过逆KL散度进行行为正则化，从而确保学习到的策略与行为策略共享相同的支撑集，避免选择分布外的动作。具体来说，目标函数中的逆KL散度项可以分解为各个智能体的独立项，这使得顺序更新每个智能体的策略成为可能。数学上通过使用Karush-Kuhn-Tucker（KKT）条件，推导出目标函数的闭式解，从而实现样本内学习。最终的优化目标为最小化KL散度，以确保策略更新的有效性和一致性。</p>
<h3 id="最大熵行为正则化马尔可夫博弈"><a href="#最大熵行为正则化马尔可夫博弈" class="headerlink" title="最大熵行为正则化马尔可夫博弈"></a>最大熵行为正则化马尔可夫博弈</h3><p>为了进一步增强探索性，防止策略过早收敛到局部最优解，InSPO引入了最大熵行为正则化马尔可夫博弈（MEBR-MG）框架。在这个框架中，策略优化的目标函数不仅包含逆KL散度项，还引入了策略熵项。通过这种方式，优化目标能够促使策略在高概率动作和低概率动作之间保持平衡，鼓励充分探索低概率动作，从而避免局部最优解。理论上，最大熵行为正则化还能够确保策略收敛到量化响应均衡（QRE），即在面对扰动奖励时，策略仍能维持稳定的性能。</p>
<p>通过上述方法，样本内顺序策略优化不仅有效解决了离线MARL中的OOD联合动作问题，还通过策略熵的引入，显著提高了策略的探索能力和全局最优解的发现概率。</p>
<h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><p><strong>算法 1: InSPO 的步骤</strong></p>
<p>InSPO算法的核心在于通过顺序更新的方式，逐步优化每个智能体的策略，最终实现全局最优。具体步骤如下：</p>
<ol>
<li>输入：离线数据集D、初始策略π0π0和初始Q函数Q0Q0。</li>
<li>输出：最终策略πKπK。</li>
<li>首先，通过简单的行为克隆方法计算出行为策略μμ。</li>
<li>接下来，开始迭代优化。在每一轮迭代中，先计算出当前Q函数QkQk。</li>
<li>随机抽取一个智能体的排列i1:Ni1:N，并依次更新每个智能体的策略。</li>
<li>对于每个智能体，使用推导出的目标函数进行策略更新。</li>
<li>重复上述过程，直到达到预定的迭代次数K。</li>
</ol>
<p>这种顺序更新的策略，确保了每一步的策略优化都是在样本内进行的，避免了分布外动作的选择，提高了策略的稳定性和有效性。</p>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><p>策略评估是InSPO算法中的一个关键步骤。根据更新的Q函数，计算当前策略的期望回报。在多智能体环境中，由于联合动作空间的庞大，研究团队采用了局部Q函数来进行近似。在策略评估过程中，需要顺序地更新每个智能体的局部Q函数，使其能反映最新的策略信息。具体的目标函数包括一个权重项，用于平衡策略的探索和利用。此外，为了降低重要性采样比率的高方差，InSPO采用了重要性重采样技术，通过概率比例重采样构建新的数据集，从而稳定算法的训练效果。</p>
<h3 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h3><p>在获得优化的局部Q函数后，接下来就是策略改进步骤。通过最小化KL散度，InSPO能够在保持行为策略特性的同时，逐步优化每个智能体的策略。在具体操作中，使用推导出的目标函数来指导每个智能体的策略更新，这一过程确保了策略的收敛性和改进性。</p>
<h3 id="实际应用及实现细节"><a href="#实际应用及实现细节" class="headerlink" title="实际应用及实现细节"></a>实际应用及实现细节</h3><p>在实际应用中，InSPO不仅需要在理论上保证策略的有效性，还需要在大规模状态-动作空间中保持高效的计算性能。为了实现这一点，论文对算法进行了多方面的优化：</p>
<ul>
<li><strong>局部Q函数的优化</strong>：为了避免联合动作空间的指数级增长，他们使用局部Q函数来近似全局Q函数，并通过顺序更新的方法逐步优化每个智能体的局部Q函数。</li>
<li><strong>重要性重采样</strong>：通过重要性重采样技术，构建新的数据集，降低采样比率的方差，提高训练的稳定性。</li>
<li><strong>自动调节温度参数α</strong>：为了找到合适的保守程度，他们实现了自动调节α的机制，根据目标值进行动态调整，从而进一步提高性能。</li>
</ul>
<p>这些优化措施使得InSPO在处理复杂的多智能体任务时，能够保持高效的性能和良好的收敛性。通过这些实际应用和实现细节，InSPO展现了其在离线MARL中的巨大潜力和应用价值。</p>
<h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>在M-NE博弈中，研究团队评估了InSPO避免收敛至局部最优的能力。实验使用两个数据集：一个是由均匀策略收集的平衡数据集，另一个是由接近局部最优的策略收集的不平衡数据集。结果显示，在平衡数据集上，大多数算法都能找到全局最优解，而在不平衡数据集上，只有InSPO正确识别出全局最优解。这表明，在存在多个局部最优解的环境中，数据集分布对算法收敛性有显著影响。InSPO通过全面探索数据集，避免了次优解的影响，展现了其强大的全局最优解识别能力。</p>
<h3 id="桥博弈的实验结果"><a href="#桥博弈的实验结果" class="headerlink" title="桥博弈的实验结果"></a>桥博弈的实验结果</h3><p>桥博弈是一个类似于时间版本XOR博弈的网格世界马尔可夫博弈。在这个实验中，他们使用了两个数据集：optimal数据集和mixed数据集。optimal数据集包含了由最优确定性策略生成的500条轨迹，而mixed数据集则包括optimal数据集和由均匀随机策略生成的额外500条轨迹。实验结果表明，只有InSPO和AlberDICE在这两个数据集上都达到了近乎最优的性能。相比之下，值分解方法未能收敛，并产生了不理想的结果。这进一步证明了InSPO在复杂多智能体任务中的有效性。</p>
<p><img src="https://p0.meituan.net/meituantechblog/2284bfa1796b4795d9018e4bf5f7e9be33448.png" alt="图3：数据集XOR博弈的最终联合策略(b)"></p>
<p>图3：数据集XOR博弈的最终联合策略(b)</p>
<h3 id="星际争霸II微操作基准测试的实验结果"><a href="#星际争霸II微操作基准测试的实验结果" class="headerlink" title="星际争霸II微操作基准测试的实验结果"></a>星际争霸II微操作基准测试的实验结果</h3><p>为了进一步验证InSPO的性能，研究团队将研究扩展到星际争霸II微操作基准测试，这是一个高维复杂的环境。实验使用了四个代表性地图，并采用了四个不同的数据集：medium、expert、medium-replay和mixed。在这些实验中，尽管值分解方法在该环境中表现出色，InSPO依然展示了其竞争力，在大多数任务中取得了最先进的结果。实验结果证明了InSPO在高维复杂环境中的应用潜力。</p>
<p><img src="https://p0.meituan.net/meituantechblog/811a42ca80594b7ac18f74d2d0ea0d57351242.png" alt="表1-4：星际争霸II微管理的平均测试获胜率"></p>
<p>表1-4：星际争霸II微管理的平均测试获胜率</p>
<p><img src="https://p0.meituan.net/meituantechblog/8cace7a43473e6a86f738daa264ff77e47957.png" alt="图4：开始时的桥"></p>
<p>图4：开始时的桥</p>
<h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><p>为了评估InSPO中不同组件的影响，研究团队进行了消融研究。首先他们在不平衡数据集上的M-NE博弈中测试了去除熵项的InSPO，结果显示没有熵扰动的InSPO无法逃离局部最优。在XOR博弈中测试了同时更新而非顺序更新的InSPO，由于更新方向的冲突，未能学习到最优策略，并面临OOD联合动作问题。此外，研究团队还评估了温度参数α对策略保守程度的影响，结果表明自动调节的α能够找到合适的值，进一步提升性能。</p>
<p><img src="https://p0.meituan.net/meituantechblog/1d7c0a491a654b42af0c7ead460237fe52663.png" alt="图5：熵消融和顺序更新方案。（a） 对于不平衡数据集，在M-NE博弈中没有熵的InSPO。（b） 是数据集（b）XOR博弈上InSPO的同步更新版本"></p>
<p>图5：熵消融和顺序更新方案。（a） 对于不平衡数据集，在M-NE博弈中没有熵的InSPO。（b） 是数据集（b）XOR博弈上InSPO的同步更新版本</p>
<p>通过这些实验验证，InSPO展现了其在解决离线MARL中的局部最优收敛问题、增强策略探索能力和提高全局最优解识别能力方面的优势。实验结果不仅证明了InSPO的理论可行性，还展示了其在实际应用中的强大潜力。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本研究中，研究团队提出了一种新颖的离线多智能体强化学习（MARL）算法——样本内顺序策略优化（In-Sample Sequential Policy Optimization, InSPO）。通过引入逆KL散度和策略熵，有效地解决了离线MARL中的分布外（OOD）联合动作和局部最优解问题。理论分析和实验验证表明，InSPO不仅能够实现策略的单调改进，并最终收敛到量化响应均衡（QRE），还在多个基准测试中展示了优越的性能。与现有的离线MARL方法相比，InSPO在应对复杂多智能体任务、提高策略稳定性和探索能力方面具有显著优势。</p>
<p>尽管InSPO在离线MARL领域取得了突破性的进展，但仍有许多值得进一步探索的方向。</p>
<ul>
<li><strong>算法扩展与优化</strong>：未来可以考虑将InSPO与其他先进的MARL算法相结合，进一步提升策略优化的效果。同时，研究如何在更大规模、更复杂的环境中实现高效的策略优化，也是一个重要的方向。</li>
<li>**数据集增强与生成：在离线MARL中，数据集的质量和分布对算法性能有着直接影响。未来可以探索通过生成对抗网络（GANs）等技术生成高质量的数据集，从而改善策略学习的效果。</li>
<li><strong>多模态奖励图谱的应对</strong>：在存在多个局部最优解的环境中，如何更有效地识别和收敛到全局最优解，仍是一个具有挑战性的问题。研究新的正则化方法和优化策略，能够进一步提升InSPO的鲁棒性。</li>
<li><strong>实际应用与验证</strong>：将InSPO应用到更多实际场景中，如智能调度、智能交通系统、自动驾驶、智能制造等，验证其在真实环境中的性能和稳定性，将是未来的重要研究方向。</li>
</ul>
<p>通过这些方向的深入研究与探索，有望进一步提升离线MARL算法的性能和应用价值，推动人工智能技术在多智能体系统中的广泛应用。希望本篇论文解读能够帮助到从事相关研究的同学。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/02/28/%E8%A1%8C%E4%B8%BA%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E9%A1%BA%E5%BA%8F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%BB%93%E5%90%88%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-id="cm7okjgcz0000g0sj9d6f5kcu" data-title="&#39;行为正则化与顺序策略优化结合的离线多智能体学习算法&#39;" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/02/28/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/28/%E8%A1%8C%E4%B8%BA%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E9%A1%BA%E5%BA%8F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%BB%93%E5%90%88%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">&#39;行为正则化与顺序策略优化结合的离线多智能体学习算法&#39;</a>
          </li>
        
          <li>
            <a href="/2025/02/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>